{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Prosta_siec_neuronowa.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/K4zu/Machine-Learning---Python/blob/master/Prosta_siec_neuronowa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-BZUP1EkH-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "X = np.array([[1.0, 0.7]])\n",
        "y_true = np.array([1.80])\n",
        "\n",
        "def initialize_parameters(n_x, n_h, n_y):\n",
        "    W1 = np.random.randn(n_x , n_h)\n",
        "    W2 = np.random.randn(n_h , n_y)\n",
        "    return W1, W2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZM-UJa90FxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, W1, W2):\n",
        "    H1 = np.dot(X, W1)\n",
        "    y_pred = np.dot(H1, W2)\n",
        "    return H1, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc7Tn5X50Haq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_error(y_pred, y_true):\n",
        "    return y_pred - y_true"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPNWdGnV0N4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, W1, W2):\n",
        "    _, y_pred = forward_propagation(X, W1, W2)\n",
        "    return y_pred[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J8we9Lw0SWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backpropagation(X, W1, W2, learning_rate, iters=1000, precision=0.0000001):\n",
        "\n",
        "    H1, y_pred = forward_propagation(X, W1, W2)\n",
        "    train_loss = []\n",
        "\n",
        "    for i in range(iters):\n",
        "        error = calculate_error(y_pred, y_true)\n",
        "        W2 = W2 - learning_rate * error * H1.T \n",
        "        W1 = W1 - learning_rate * error * np.dot(X.T, W2.T)\n",
        "\n",
        "        y_pred = predict(X, W1, W2)\n",
        "        print(f'Iter #{i}: y_pred {y_pred}: loss: {abs(calculate_error(y_pred, y_true[0]))}')\n",
        "        train_loss.append(abs(calculate_error(y_pred, y_true[0])))\n",
        "\n",
        "        if abs(error) < precision:\n",
        "            break\n",
        "\n",
        "    return W1, W2, train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ85CQmB0OkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model():\n",
        "\n",
        "    W1, W2 = initialize_parameters(2, 2, 1)\n",
        "    \n",
        "    W1, W2, train_loss = backpropagation(X, W1, W2, 0.01)\n",
        "\n",
        "    model = {'W1': W1, 'W2': W2, 'train_loss': train_loss}\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQzF1fBdrMAf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "25274648-37a4-44eb-d3af-f8e5309b2344"
      },
      "source": [
        "model = build_model()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iter #0: y_pred [-0.39718278]: loss: [2.19718278]\n",
            "Iter #1: y_pred [-0.3563008]: loss: [2.1563008]\n",
            "Iter #2: y_pred [-0.31690916]: loss: [2.11690916]\n",
            "Iter #3: y_pred [-0.27886411]: loss: [2.07886411]\n",
            "Iter #4: y_pred [-0.24203526]: loss: [2.04203526]\n",
            "Iter #5: y_pred [-0.20630423]: loss: [2.00630423]\n",
            "Iter #6: y_pred [-0.17156348]: loss: [1.97156348]\n",
            "Iter #7: y_pred [-0.13771532]: loss: [1.93771532]\n",
            "Iter #8: y_pred [-0.10467103]: loss: [1.90467103]\n",
            "Iter #9: y_pred [-0.07235002]: loss: [1.87235002]\n",
            "Iter #10: y_pred [-0.04067924]: loss: [1.84067924]\n",
            "Iter #11: y_pred [-0.0095925]: loss: [1.8095925]\n",
            "Iter #12: y_pred [0.02097004]: loss: [1.77902996]\n",
            "Iter #13: y_pred [0.05106234]: loss: [1.74893766]\n",
            "Iter #14: y_pred [0.08073293]: loss: [1.71926707]\n",
            "Iter #15: y_pred [0.11002529]: loss: [1.68997471]\n",
            "Iter #16: y_pred [0.13897819]: loss: [1.66102181]\n",
            "Iter #17: y_pred [0.16762601]: loss: [1.63237399]\n",
            "Iter #18: y_pred [0.19599907]: loss: [1.60400093]\n",
            "Iter #19: y_pred [0.22412386]: loss: [1.57587614]\n",
            "Iter #20: y_pred [0.25202329]: loss: [1.54797671]\n",
            "Iter #21: y_pred [0.27971697]: loss: [1.52028303]\n",
            "Iter #22: y_pred [0.3072214]: loss: [1.4927786]\n",
            "Iter #23: y_pred [0.33455016]: loss: [1.46544984]\n",
            "Iter #24: y_pred [0.36171416]: loss: [1.43828584]\n",
            "Iter #25: y_pred [0.38872181]: loss: [1.41127819]\n",
            "Iter #26: y_pred [0.41557916]: loss: [1.38442084]\n",
            "Iter #27: y_pred [0.44229017]: loss: [1.35770983]\n",
            "Iter #28: y_pred [0.46885678]: loss: [1.33114322]\n",
            "Iter #29: y_pred [0.49527916]: loss: [1.30472084]\n",
            "Iter #30: y_pred [0.5215558]: loss: [1.2784442]\n",
            "Iter #31: y_pred [0.54768372]: loss: [1.25231628]\n",
            "Iter #32: y_pred [0.57365859]: loss: [1.22634141]\n",
            "Iter #33: y_pred [0.59947488]: loss: [1.20052512]\n",
            "Iter #34: y_pred [0.62512602]: loss: [1.17487398]\n",
            "Iter #35: y_pred [0.65060451]: loss: [1.14939549]\n",
            "Iter #36: y_pred [0.67590207]: loss: [1.12409793]\n",
            "Iter #37: y_pred [0.70100975]: loss: [1.09899025]\n",
            "Iter #38: y_pred [0.72591808]: loss: [1.07408192]\n",
            "Iter #39: y_pred [0.75061716]: loss: [1.04938284]\n",
            "Iter #40: y_pred [0.77509677]: loss: [1.02490323]\n",
            "Iter #41: y_pred [0.79934648]: loss: [1.00065352]\n",
            "Iter #42: y_pred [0.82335576]: loss: [0.97664424]\n",
            "Iter #43: y_pred [0.84711407]: loss: [0.95288593]\n",
            "Iter #44: y_pred [0.87061092]: loss: [0.92938908]\n",
            "Iter #45: y_pred [0.89383597]: loss: [0.90616403]\n",
            "Iter #46: y_pred [0.91677911]: loss: [0.88322089]\n",
            "Iter #47: y_pred [0.9394305]: loss: [0.8605695]\n",
            "Iter #48: y_pred [0.96178068]: loss: [0.83821932]\n",
            "Iter #49: y_pred [0.98382055]: loss: [0.81617945]\n",
            "Iter #50: y_pred [1.00554148]: loss: [0.79445852]\n",
            "Iter #51: y_pred [1.02693534]: loss: [0.77306466]\n",
            "Iter #52: y_pred [1.04799449]: loss: [0.75200551]\n",
            "Iter #53: y_pred [1.06871187]: loss: [0.73128813]\n",
            "Iter #54: y_pred [1.08908101]: loss: [0.71091899]\n",
            "Iter #55: y_pred [1.109096]: loss: [0.690904]\n",
            "Iter #56: y_pred [1.12875158]: loss: [0.67124842]\n",
            "Iter #57: y_pred [1.14804308]: loss: [0.65195692]\n",
            "Iter #58: y_pred [1.16696647]: loss: [0.63303353]\n",
            "Iter #59: y_pred [1.18551833]: loss: [0.61448167]\n",
            "Iter #60: y_pred [1.20369588]: loss: [0.59630412]\n",
            "Iter #61: y_pred [1.22149695]: loss: [0.57850305]\n",
            "Iter #62: y_pred [1.23891996]: loss: [0.56108004]\n",
            "Iter #63: y_pred [1.25596393]: loss: [0.54403607]\n",
            "Iter #64: y_pred [1.27262846]: loss: [0.52737154]\n",
            "Iter #65: y_pred [1.2889137]: loss: [0.5110863]\n",
            "Iter #66: y_pred [1.30482033]: loss: [0.49517967]\n",
            "Iter #67: y_pred [1.32034957]: loss: [0.47965043]\n",
            "Iter #68: y_pred [1.3355031]: loss: [0.4644969]\n",
            "Iter #69: y_pred [1.35028311]: loss: [0.44971689]\n",
            "Iter #70: y_pred [1.3646922]: loss: [0.4353078]\n",
            "Iter #71: y_pred [1.3787334]: loss: [0.4212666]\n",
            "Iter #72: y_pred [1.39241014]: loss: [0.40758986]\n",
            "Iter #73: y_pred [1.40572621]: loss: [0.39427379]\n",
            "Iter #74: y_pred [1.41868575]: loss: [0.38131425]\n",
            "Iter #75: y_pred [1.4312932]: loss: [0.3687068]\n",
            "Iter #76: y_pred [1.44355331]: loss: [0.35644669]\n",
            "Iter #77: y_pred [1.45547107]: loss: [0.34452893]\n",
            "Iter #78: y_pred [1.46705173]: loss: [0.33294827]\n",
            "Iter #79: y_pred [1.47830074]: loss: [0.32169926]\n",
            "Iter #80: y_pred [1.48922375]: loss: [0.31077625]\n",
            "Iter #81: y_pred [1.49982659]: loss: [0.30017341]\n",
            "Iter #82: y_pred [1.5101152]: loss: [0.2898848]\n",
            "Iter #83: y_pred [1.5200957]: loss: [0.2799043]\n",
            "Iter #84: y_pred [1.52977427]: loss: [0.27022573]\n",
            "Iter #85: y_pred [1.53915719]: loss: [0.26084281]\n",
            "Iter #86: y_pred [1.54825082]: loss: [0.25174918]\n",
            "Iter #87: y_pred [1.55706156]: loss: [0.24293844]\n",
            "Iter #88: y_pred [1.56559584]: loss: [0.23440416]\n",
            "Iter #89: y_pred [1.57386014]: loss: [0.22613986]\n",
            "Iter #90: y_pred [1.58186091]: loss: [0.21813909]\n",
            "Iter #91: y_pred [1.58960461]: loss: [0.21039539]\n",
            "Iter #92: y_pred [1.59709767]: loss: [0.20290233]\n",
            "Iter #93: y_pred [1.6043465]: loss: [0.1956535]\n",
            "Iter #94: y_pred [1.61135747]: loss: [0.18864253]\n",
            "Iter #95: y_pred [1.61813688]: loss: [0.18186312]\n",
            "Iter #96: y_pred [1.624691]: loss: [0.175309]\n",
            "Iter #97: y_pred [1.63102599]: loss: [0.16897401]\n",
            "Iter #98: y_pred [1.63714797]: loss: [0.16285203]\n",
            "Iter #99: y_pred [1.64306296]: loss: [0.15693704]\n",
            "Iter #100: y_pred [1.64877691]: loss: [0.15122309]\n",
            "Iter #101: y_pred [1.65429564]: loss: [0.14570436]\n",
            "Iter #102: y_pred [1.65962492]: loss: [0.14037508]\n",
            "Iter #103: y_pred [1.66477037]: loss: [0.13522963]\n",
            "Iter #104: y_pred [1.66973755]: loss: [0.13026245]\n",
            "Iter #105: y_pred [1.67453187]: loss: [0.12546813]\n",
            "Iter #106: y_pred [1.67915866]: loss: [0.12084134]\n",
            "Iter #107: y_pred [1.68362312]: loss: [0.11637688]\n",
            "Iter #108: y_pred [1.68793034]: loss: [0.11206966]\n",
            "Iter #109: y_pred [1.69208531]: loss: [0.10791469]\n",
            "Iter #110: y_pred [1.69609287]: loss: [0.10390713]\n",
            "Iter #111: y_pred [1.69995778]: loss: [0.10004222]\n",
            "Iter #112: y_pred [1.70368466]: loss: [0.09631534]\n",
            "Iter #113: y_pred [1.70727802]: loss: [0.09272198]\n",
            "Iter #114: y_pred [1.71074224]: loss: [0.08925776]\n",
            "Iter #115: y_pred [1.71408162]: loss: [0.08591838]\n",
            "Iter #116: y_pred [1.71730031]: loss: [0.08269969]\n",
            "Iter #117: y_pred [1.72040236]: loss: [0.07959764]\n",
            "Iter #118: y_pred [1.7233917]: loss: [0.0766083]\n",
            "Iter #119: y_pred [1.72627217]: loss: [0.07372783]\n",
            "Iter #120: y_pred [1.72904747]: loss: [0.07095253]\n",
            "Iter #121: y_pred [1.73172121]: loss: [0.06827879]\n",
            "Iter #122: y_pred [1.7342969]: loss: [0.0657031]\n",
            "Iter #123: y_pred [1.73677793]: loss: [0.06322207]\n",
            "Iter #124: y_pred [1.73916759]: loss: [0.06083241]\n",
            "Iter #125: y_pred [1.74146907]: loss: [0.05853093]\n",
            "Iter #126: y_pred [1.74368547]: loss: [0.05631453]\n",
            "Iter #127: y_pred [1.74581979]: loss: [0.05418021]\n",
            "Iter #128: y_pred [1.74787493]: loss: [0.05212507]\n",
            "Iter #129: y_pred [1.74985369]: loss: [0.05014631]\n",
            "Iter #130: y_pred [1.7517588]: loss: [0.0482412]\n",
            "Iter #131: y_pred [1.7535929]: loss: [0.0464071]\n",
            "Iter #132: y_pred [1.75535851]: loss: [0.04464149]\n",
            "Iter #133: y_pred [1.75705812]: loss: [0.04294188]\n",
            "Iter #134: y_pred [1.7586941]: loss: [0.0413059]\n",
            "Iter #135: y_pred [1.76026875]: loss: [0.03973125]\n",
            "Iter #136: y_pred [1.76178429]: loss: [0.03821571]\n",
            "Iter #137: y_pred [1.76324288]: loss: [0.03675712]\n",
            "Iter #138: y_pred [1.76464659]: loss: [0.03535341]\n",
            "Iter #139: y_pred [1.76599742]: loss: [0.03400258]\n",
            "Iter #140: y_pred [1.76729732]: loss: [0.03270268]\n",
            "Iter #141: y_pred [1.76854815]: loss: [0.03145185]\n",
            "Iter #142: y_pred [1.76975172]: loss: [0.03024828]\n",
            "Iter #143: y_pred [1.77090977]: loss: [0.02909023]\n",
            "Iter #144: y_pred [1.77202398]: loss: [0.02797602]\n",
            "Iter #145: y_pred [1.77309598]: loss: [0.02690402]\n",
            "Iter #146: y_pred [1.77412732]: loss: [0.02587268]\n",
            "Iter #147: y_pred [1.77511952]: loss: [0.02488048]\n",
            "Iter #148: y_pred [1.77607404]: loss: [0.02392596]\n",
            "Iter #149: y_pred [1.77699227]: loss: [0.02300773]\n",
            "Iter #150: y_pred [1.77787557]: loss: [0.02212443]\n",
            "Iter #151: y_pred [1.77872525]: loss: [0.02127475]\n",
            "Iter #152: y_pred [1.77954257]: loss: [0.02045743]\n",
            "Iter #153: y_pred [1.78032873]: loss: [0.01967127]\n",
            "Iter #154: y_pred [1.78108491]: loss: [0.01891509]\n",
            "Iter #155: y_pred [1.78181223]: loss: [0.01818777]\n",
            "Iter #156: y_pred [1.78251178]: loss: [0.01748822]\n",
            "Iter #157: y_pred [1.78318461]: loss: [0.01681539]\n",
            "Iter #158: y_pred [1.78383171]: loss: [0.01616829]\n",
            "Iter #159: y_pred [1.78445407]: loss: [0.01554593]\n",
            "Iter #160: y_pred [1.78505262]: loss: [0.01494738]\n",
            "Iter #161: y_pred [1.78562825]: loss: [0.01437175]\n",
            "Iter #162: y_pred [1.78618183]: loss: [0.01381817]\n",
            "Iter #163: y_pred [1.7867142]: loss: [0.0132858]\n",
            "Iter #164: y_pred [1.78722617]: loss: [0.01277383]\n",
            "Iter #165: y_pred [1.78771851]: loss: [0.01228149]\n",
            "Iter #166: y_pred [1.78819196]: loss: [0.01180804]\n",
            "Iter #167: y_pred [1.78864724]: loss: [0.01135276]\n",
            "Iter #168: y_pred [1.78908504]: loss: [0.01091496]\n",
            "Iter #169: y_pred [1.78950603]: loss: [0.01049397]\n",
            "Iter #170: y_pred [1.78991084]: loss: [0.01008916]\n",
            "Iter #171: y_pred [1.7903001]: loss: [0.0096999]\n",
            "Iter #172: y_pred [1.7906744]: loss: [0.0093256]\n",
            "Iter #173: y_pred [1.79103431]: loss: [0.00896569]\n",
            "Iter #174: y_pred [1.79138037]: loss: [0.00861963]\n",
            "Iter #175: y_pred [1.79171312]: loss: [0.00828688]\n",
            "Iter #176: y_pred [1.79203306]: loss: [0.00796694]\n",
            "Iter #177: y_pred [1.79234069]: loss: [0.00765931]\n",
            "Iter #178: y_pred [1.79263648]: loss: [0.00736352]\n",
            "Iter #179: y_pred [1.79292088]: loss: [0.00707912]\n",
            "Iter #180: y_pred [1.79319432]: loss: [0.00680568]\n",
            "Iter #181: y_pred [1.79345722]: loss: [0.00654278]\n",
            "Iter #182: y_pred [1.79371]: loss: [0.00629]\n",
            "Iter #183: y_pred [1.79395303]: loss: [0.00604697]\n",
            "Iter #184: y_pred [1.7941867]: loss: [0.0058133]\n",
            "Iter #185: y_pred [1.79441135]: loss: [0.00558865]\n",
            "Iter #186: y_pred [1.79462734]: loss: [0.00537266]\n",
            "Iter #187: y_pred [1.79483501]: loss: [0.00516499]\n",
            "Iter #188: y_pred [1.79503466]: loss: [0.00496534]\n",
            "Iter #189: y_pred [1.7952266]: loss: [0.0047734]\n",
            "Iter #190: y_pred [1.79541115]: loss: [0.00458885]\n",
            "Iter #191: y_pred [1.79558857]: loss: [0.00441143]\n",
            "Iter #192: y_pred [1.79575914]: loss: [0.00424086]\n",
            "Iter #193: y_pred [1.79592312]: loss: [0.00407688]\n",
            "Iter #194: y_pred [1.79608078]: loss: [0.00391922]\n",
            "Iter #195: y_pred [1.79623235]: loss: [0.00376765]\n",
            "Iter #196: y_pred [1.79637806]: loss: [0.00362194]\n",
            "Iter #197: y_pred [1.79651815]: loss: [0.00348185]\n",
            "Iter #198: y_pred [1.79665282]: loss: [0.00334718]\n",
            "Iter #199: y_pred [1.7967823]: loss: [0.0032177]\n",
            "Iter #200: y_pred [1.79690677]: loss: [0.00309323]\n",
            "Iter #201: y_pred [1.79702643]: loss: [0.00297357]\n",
            "Iter #202: y_pred [1.79714147]: loss: [0.00285853]\n",
            "Iter #203: y_pred [1.79725206]: loss: [0.00274794]\n",
            "Iter #204: y_pred [1.79735838]: loss: [0.00264162]\n",
            "Iter #205: y_pred [1.79746059]: loss: [0.00253941]\n",
            "Iter #206: y_pred [1.79755885]: loss: [0.00244115]\n",
            "Iter #207: y_pred [1.79765331]: loss: [0.00234669]\n",
            "Iter #208: y_pred [1.79774411]: loss: [0.00225589]\n",
            "Iter #209: y_pred [1.79783141]: loss: [0.00216859]\n",
            "Iter #210: y_pred [1.79791533]: loss: [0.00208467]\n",
            "Iter #211: y_pred [1.79799601]: loss: [0.00200399]\n",
            "Iter #212: y_pred [1.79807357]: loss: [0.00192643]\n",
            "Iter #213: y_pred [1.79814812]: loss: [0.00185188]\n",
            "Iter #214: y_pred [1.7982198]: loss: [0.0017802]\n",
            "Iter #215: y_pred [1.7982887]: loss: [0.0017113]\n",
            "Iter #216: y_pred [1.79835494]: loss: [0.00164506]\n",
            "Iter #217: y_pred [1.79841861]: loss: [0.00158139]\n",
            "Iter #218: y_pred [1.79847982]: loss: [0.00152018]\n",
            "Iter #219: y_pred [1.79853867]: loss: [0.00146133]\n",
            "Iter #220: y_pred [1.79859523]: loss: [0.00140477]\n",
            "Iter #221: y_pred [1.79864961]: loss: [0.00135039]\n",
            "Iter #222: y_pred [1.79870189]: loss: [0.00129811]\n",
            "Iter #223: y_pred [1.79875214]: loss: [0.00124786]\n",
            "Iter #224: y_pred [1.79880045]: loss: [0.00119955]\n",
            "Iter #225: y_pred [1.79884688]: loss: [0.00115312]\n",
            "Iter #226: y_pred [1.79889153]: loss: [0.00110847]\n",
            "Iter #227: y_pred [1.79893444]: loss: [0.00106556]\n",
            "Iter #228: y_pred [1.79897569]: loss: [0.00102431]\n",
            "Iter #229: y_pred [1.79901535]: loss: [0.00098465]\n",
            "Iter #230: y_pred [1.79905347]: loss: [0.00094653]\n",
            "Iter #231: y_pred [1.79909012]: loss: [0.00090988]\n",
            "Iter #232: y_pred [1.79912535]: loss: [0.00087465]\n",
            "Iter #233: y_pred [1.79915921]: loss: [0.00084079]\n",
            "Iter #234: y_pred [1.79919176]: loss: [0.00080824]\n",
            "Iter #235: y_pred [1.79922306]: loss: [0.00077694]\n",
            "Iter #236: y_pred [1.79925314]: loss: [0.00074686]\n",
            "Iter #237: y_pred [1.79928206]: loss: [0.00071794]\n",
            "Iter #238: y_pred [1.79930986]: loss: [0.00069014]\n",
            "Iter #239: y_pred [1.79933658]: loss: [0.00066342]\n",
            "Iter #240: y_pred [1.79936227]: loss: [0.00063773]\n",
            "Iter #241: y_pred [1.79938696]: loss: [0.00061304]\n",
            "Iter #242: y_pred [1.7994107]: loss: [0.0005893]\n",
            "Iter #243: y_pred [1.79943352]: loss: [0.00056648]\n",
            "Iter #244: y_pred [1.79945545]: loss: [0.00054455]\n",
            "Iter #245: y_pred [1.79947654]: loss: [0.00052346]\n",
            "Iter #246: y_pred [1.79949681]: loss: [0.00050319]\n",
            "Iter #247: y_pred [1.7995163]: loss: [0.0004837]\n",
            "Iter #248: y_pred [1.79953503]: loss: [0.00046497]\n",
            "Iter #249: y_pred [1.79955303]: loss: [0.00044697]\n",
            "Iter #250: y_pred [1.79957034]: loss: [0.00042966]\n",
            "Iter #251: y_pred [1.79958698]: loss: [0.00041302]\n",
            "Iter #252: y_pred [1.79960297]: loss: [0.00039703]\n",
            "Iter #253: y_pred [1.79961835]: loss: [0.00038165]\n",
            "Iter #254: y_pred [1.79963313]: loss: [0.00036687]\n",
            "Iter #255: y_pred [1.79964733]: loss: [0.00035267]\n",
            "Iter #256: y_pred [1.79966099]: loss: [0.00033901]\n",
            "Iter #257: y_pred [1.79967412]: loss: [0.00032588]\n",
            "Iter #258: y_pred [1.79968674]: loss: [0.00031326]\n",
            "Iter #259: y_pred [1.79969887]: loss: [0.00030113]\n",
            "Iter #260: y_pred [1.79971053]: loss: [0.00028947]\n",
            "Iter #261: y_pred [1.79972174]: loss: [0.00027826]\n",
            "Iter #262: y_pred [1.79973252]: loss: [0.00026748]\n",
            "Iter #263: y_pred [1.79974288]: loss: [0.00025712]\n",
            "Iter #264: y_pred [1.79975284]: loss: [0.00024716]\n",
            "Iter #265: y_pred [1.79976241]: loss: [0.00023759]\n",
            "Iter #266: y_pred [1.79977161]: loss: [0.00022839]\n",
            "Iter #267: y_pred [1.79978045]: loss: [0.00021955]\n",
            "Iter #268: y_pred [1.79978896]: loss: [0.00021104]\n",
            "Iter #269: y_pred [1.79979713]: loss: [0.00020287]\n",
            "Iter #270: y_pred [1.79980499]: loss: [0.00019501]\n",
            "Iter #271: y_pred [1.79981254]: loss: [0.00018746]\n",
            "Iter #272: y_pred [1.7998198]: loss: [0.0001802]\n",
            "Iter #273: y_pred [1.79982678]: loss: [0.00017322]\n",
            "Iter #274: y_pred [1.79983349]: loss: [0.00016651]\n",
            "Iter #275: y_pred [1.79983993]: loss: [0.00016007]\n",
            "Iter #276: y_pred [1.79984613]: loss: [0.00015387]\n",
            "Iter #277: y_pred [1.79985209]: loss: [0.00014791]\n",
            "Iter #278: y_pred [1.79985782]: loss: [0.00014218]\n",
            "Iter #279: y_pred [1.79986333]: loss: [0.00013667]\n",
            "Iter #280: y_pred [1.79986862]: loss: [0.00013138]\n",
            "Iter #281: y_pred [1.79987371]: loss: [0.00012629]\n",
            "Iter #282: y_pred [1.7998786]: loss: [0.0001214]\n",
            "Iter #283: y_pred [1.7998833]: loss: [0.0001167]\n",
            "Iter #284: y_pred [1.79988782]: loss: [0.00011218]\n",
            "Iter #285: y_pred [1.79989217]: loss: [0.00010783]\n",
            "Iter #286: y_pred [1.79989634]: loss: [0.00010366]\n",
            "Iter #287: y_pred [1.79990036]: loss: [9.96434757e-05]\n",
            "Iter #288: y_pred [1.79990422]: loss: [9.57843619e-05]\n",
            "Iter #289: y_pred [1.79990793]: loss: [9.20747031e-05]\n",
            "Iter #290: y_pred [1.79991149]: loss: [8.85087116e-05]\n",
            "Iter #291: y_pred [1.79991492]: loss: [8.50808239e-05]\n",
            "Iter #292: y_pred [1.79991821]: loss: [8.1785692e-05]\n",
            "Iter #293: y_pred [1.79992138]: loss: [7.86181746e-05]\n",
            "Iter #294: y_pred [1.79992443]: loss: [7.55733299e-05]\n",
            "Iter #295: y_pred [1.79992735]: loss: [7.26464071e-05]\n",
            "Iter #296: y_pred [1.79993017]: loss: [6.98328398e-05]\n",
            "Iter #297: y_pred [1.79993287]: loss: [6.71282379e-05]\n",
            "Iter #298: y_pred [1.79993547]: loss: [6.45283816e-05]\n",
            "Iter #299: y_pred [1.79993797]: loss: [6.20292145e-05]\n",
            "Iter #300: y_pred [1.79994037]: loss: [5.96268372e-05]\n",
            "Iter #301: y_pred [1.79994268]: loss: [5.73175013e-05]\n",
            "Iter #302: y_pred [1.7999449]: loss: [5.50976036e-05]\n",
            "Iter #303: y_pred [1.79994704]: loss: [5.29636804e-05]\n",
            "Iter #304: y_pred [1.79994909]: loss: [5.09124021e-05]\n",
            "Iter #305: y_pred [1.79995106]: loss: [4.8940568e-05]\n",
            "Iter #306: y_pred [1.79995295]: loss: [4.70451016e-05]\n",
            "Iter #307: y_pred [1.79995478]: loss: [4.52230452e-05]\n",
            "Iter #308: y_pred [1.79995653]: loss: [4.34715558e-05]\n",
            "Iter #309: y_pred [1.79995821]: loss: [4.17879005e-05]\n",
            "Iter #310: y_pred [1.79995983]: loss: [4.01694522e-05]\n",
            "Iter #311: y_pred [1.79996139]: loss: [3.86136856e-05]\n",
            "Iter #312: y_pred [1.79996288]: loss: [3.7118173e-05]\n",
            "Iter #313: y_pred [1.79996432]: loss: [3.5680581e-05]\n",
            "Iter #314: y_pred [1.7999657]: loss: [3.42986665e-05]\n",
            "Iter #315: y_pred [1.79996703]: loss: [3.29702729e-05]\n",
            "Iter #316: y_pred [1.79996831]: loss: [3.16933277e-05]\n",
            "Iter #317: y_pred [1.79996953]: loss: [3.04658382e-05]\n",
            "Iter #318: y_pred [1.79997071]: loss: [2.92858891e-05]\n",
            "Iter #319: y_pred [1.79997185]: loss: [2.81516392e-05]\n",
            "Iter #320: y_pred [1.79997294]: loss: [2.70613186e-05]\n",
            "Iter #321: y_pred [1.79997399]: loss: [2.6013226e-05]\n",
            "Iter #322: y_pred [1.79997499]: loss: [2.50057259e-05]\n",
            "Iter #323: y_pred [1.79997596]: loss: [2.40372462e-05]\n",
            "Iter #324: y_pred [1.79997689]: loss: [2.31062757e-05]\n",
            "Iter #325: y_pred [1.79997779]: loss: [2.22113617e-05]\n",
            "Iter #326: y_pred [1.79997865]: loss: [2.13511077e-05]\n",
            "Iter #327: y_pred [1.79997948]: loss: [2.05241714e-05]\n",
            "Iter #328: y_pred [1.79998027]: loss: [1.97292623e-05]\n",
            "Iter #329: y_pred [1.79998103]: loss: [1.89651402e-05]\n",
            "Iter #330: y_pred [1.79998177]: loss: [1.82306127e-05]\n",
            "Iter #331: y_pred [1.79998248]: loss: [1.75245334e-05]\n",
            "Iter #332: y_pred [1.79998315]: loss: [1.68458008e-05]\n",
            "Iter #333: y_pred [1.79998381]: loss: [1.61933556e-05]\n",
            "Iter #334: y_pred [1.79998443]: loss: [1.55661797e-05]\n",
            "Iter #335: y_pred [1.79998504]: loss: [1.49632944e-05]\n",
            "Iter #336: y_pred [1.79998562]: loss: [1.43837591e-05]\n",
            "Iter #337: y_pred [1.79998617]: loss: [1.38266693e-05]\n",
            "Iter #338: y_pred [1.79998671]: loss: [1.32911557e-05]\n",
            "Iter #339: y_pred [1.79998722]: loss: [1.27763828e-05]\n",
            "Iter #340: y_pred [1.79998772]: loss: [1.22815471e-05]\n",
            "Iter #341: y_pred [1.79998819]: loss: [1.18058766e-05]\n",
            "Iter #342: y_pred [1.79998865]: loss: [1.1348629e-05]\n",
            "Iter #343: y_pred [1.79998909]: loss: [1.09090907e-05]\n",
            "Iter #344: y_pred [1.79998951]: loss: [1.04865759e-05]\n",
            "Iter #345: y_pred [1.79998992]: loss: [1.00804252e-05]\n",
            "Iter #346: y_pred [1.79999031]: loss: [9.69000497e-06]\n",
            "Iter #347: y_pred [1.79999069]: loss: [9.31470582e-06]\n",
            "Iter #348: y_pred [1.79999105]: loss: [8.95394216e-06]\n",
            "Iter #349: y_pred [1.79999139]: loss: [8.60715103e-06]\n",
            "Iter #350: y_pred [1.79999173]: loss: [8.27379127e-06]\n",
            "Iter #351: y_pred [1.79999205]: loss: [7.95334267e-06]\n",
            "Iter #352: y_pred [1.79999235]: loss: [7.64530519e-06]\n",
            "Iter #353: y_pred [1.79999265]: loss: [7.34919815e-06]\n",
            "Iter #354: y_pred [1.79999294]: loss: [7.06455947e-06]\n",
            "Iter #355: y_pred [1.79999321]: loss: [6.79094497e-06]\n",
            "Iter #356: y_pred [1.79999347]: loss: [6.5279277e-06]\n",
            "Iter #357: y_pred [1.79999372]: loss: [6.27509722e-06]\n",
            "Iter #358: y_pred [1.79999397]: loss: [6.03205899e-06]\n",
            "Iter #359: y_pred [1.7999942]: loss: [5.79843374e-06]\n",
            "Iter #360: y_pred [1.79999443]: loss: [5.57385693e-06]\n",
            "Iter #361: y_pred [1.79999464]: loss: [5.35797809e-06]\n",
            "Iter #362: y_pred [1.79999485]: loss: [5.15046035e-06]\n",
            "Iter #363: y_pred [1.79999505]: loss: [4.95097989e-06]\n",
            "Iter #364: y_pred [1.79999524]: loss: [4.7592254e-06]\n",
            "Iter #365: y_pred [1.79999543]: loss: [4.57489768e-06]\n",
            "Iter #366: y_pred [1.7999956]: loss: [4.39770907e-06]\n",
            "Iter #367: y_pred [1.79999577]: loss: [4.22738307e-06]\n",
            "Iter #368: y_pred [1.79999594]: loss: [4.06365389e-06]\n",
            "Iter #369: y_pred [1.79999609]: loss: [3.90626603e-06]\n",
            "Iter #370: y_pred [1.79999625]: loss: [3.7549739e-06]\n",
            "Iter #371: y_pred [1.79999639]: loss: [3.60954139e-06]\n",
            "Iter #372: y_pred [1.79999653]: loss: [3.46974158e-06]\n",
            "Iter #373: y_pred [1.79999666]: loss: [3.33535629e-06]\n",
            "Iter #374: y_pred [1.79999679]: loss: [3.20617582e-06]\n",
            "Iter #375: y_pred [1.79999692]: loss: [3.08199858e-06]\n",
            "Iter #376: y_pred [1.79999704]: loss: [2.96263081e-06]\n",
            "Iter #377: y_pred [1.79999715]: loss: [2.84788621e-06]\n",
            "Iter #378: y_pred [1.79999726]: loss: [2.73758575e-06]\n",
            "Iter #379: y_pred [1.79999737]: loss: [2.63155729e-06]\n",
            "Iter #380: y_pred [1.79999747]: loss: [2.52963538e-06]\n",
            "Iter #381: y_pred [1.79999757]: loss: [2.43166096e-06]\n",
            "Iter #382: y_pred [1.79999766]: loss: [2.33748115e-06]\n",
            "Iter #383: y_pred [1.79999775]: loss: [2.24694899e-06]\n",
            "Iter #384: y_pred [1.79999784]: loss: [2.15992319e-06]\n",
            "Iter #385: y_pred [1.79999792]: loss: [2.07626795e-06]\n",
            "Iter #386: y_pred [1.799998]: loss: [1.99585273e-06]\n",
            "Iter #387: y_pred [1.79999808]: loss: [1.91855205e-06]\n",
            "Iter #388: y_pred [1.79999816]: loss: [1.84424527e-06]\n",
            "Iter #389: y_pred [1.79999823]: loss: [1.77281644e-06]\n",
            "Iter #390: y_pred [1.7999983]: loss: [1.70415409e-06]\n",
            "Iter #391: y_pred [1.79999836]: loss: [1.63815108e-06]\n",
            "Iter #392: y_pred [1.79999843]: loss: [1.5747044e-06]\n",
            "Iter #393: y_pred [1.79999849]: loss: [1.51371506e-06]\n",
            "Iter #394: y_pred [1.79999854]: loss: [1.45508787e-06]\n",
            "Iter #395: y_pred [1.7999986]: loss: [1.39873135e-06]\n",
            "Iter #396: y_pred [1.79999866]: loss: [1.34455756e-06]\n",
            "Iter #397: y_pred [1.79999871]: loss: [1.29248195e-06]\n",
            "Iter #398: y_pred [1.79999876]: loss: [1.24242326e-06]\n",
            "Iter #399: y_pred [1.79999881]: loss: [1.19430338e-06]\n",
            "Iter #400: y_pred [1.79999885]: loss: [1.14804721e-06]\n",
            "Iter #401: y_pred [1.7999989]: loss: [1.10358258e-06]\n",
            "Iter #402: y_pred [1.79999894]: loss: [1.06084009e-06]\n",
            "Iter #403: y_pred [1.79999898]: loss: [1.01975304e-06]\n",
            "Iter #404: y_pred [1.79999902]: loss: [9.80257323e-07]\n",
            "Iter #405: y_pred [1.79999906]: loss: [9.42291299e-07]\n",
            "Iter #406: y_pred [1.79999909]: loss: [9.05795725e-07]\n",
            "Iter #407: y_pred [1.79999913]: loss: [8.70713648e-07]\n",
            "Iter #408: y_pred [1.79999916]: loss: [8.36990323e-07]\n",
            "Iter #409: y_pred [1.7999992]: loss: [8.04573125e-07]\n",
            "Iter #410: y_pred [1.79999923]: loss: [7.73411466e-07]\n",
            "Iter #411: y_pred [1.79999926]: loss: [7.43456719e-07]\n",
            "Iter #412: y_pred [1.79999929]: loss: [7.14662139e-07]\n",
            "Iter #413: y_pred [1.79999931]: loss: [6.86982793e-07]\n",
            "Iter #414: y_pred [1.79999934]: loss: [6.60375486e-07]\n",
            "Iter #415: y_pred [1.79999937]: loss: [6.34798697e-07]\n",
            "Iter #416: y_pred [1.79999939]: loss: [6.10212515e-07]\n",
            "Iter #417: y_pred [1.79999941]: loss: [5.86578571e-07]\n",
            "Iter #418: y_pred [1.79999944]: loss: [5.63859986e-07]\n",
            "Iter #419: y_pred [1.79999946]: loss: [5.42021308e-07]\n",
            "Iter #420: y_pred [1.79999948]: loss: [5.21028455e-07]\n",
            "Iter #421: y_pred [1.7999995]: loss: [5.00848671e-07]\n",
            "Iter #422: y_pred [1.79999952]: loss: [4.81450462e-07]\n",
            "Iter #423: y_pred [1.79999954]: loss: [4.6280356e-07]\n",
            "Iter #424: y_pred [1.79999956]: loss: [4.44878864e-07]\n",
            "Iter #425: y_pred [1.79999957]: loss: [4.27648404e-07]\n",
            "Iter #426: y_pred [1.79999959]: loss: [4.11085291e-07]\n",
            "Iter #427: y_pred [1.7999996]: loss: [3.95163678e-07]\n",
            "Iter #428: y_pred [1.79999962]: loss: [3.79858721e-07]\n",
            "Iter #429: y_pred [1.79999963]: loss: [3.65146535e-07]\n",
            "Iter #430: y_pred [1.79999965]: loss: [3.51004161e-07]\n",
            "Iter #431: y_pred [1.79999966]: loss: [3.37409531e-07]\n",
            "Iter #432: y_pred [1.79999968]: loss: [3.24341431e-07]\n",
            "Iter #433: y_pred [1.79999969]: loss: [3.11779467e-07]\n",
            "Iter #434: y_pred [1.7999997]: loss: [2.99704036e-07]\n",
            "Iter #435: y_pred [1.79999971]: loss: [2.88096294e-07]\n",
            "Iter #436: y_pred [1.79999972]: loss: [2.76938129e-07]\n",
            "Iter #437: y_pred [1.79999973]: loss: [2.66212126e-07]\n",
            "Iter #438: y_pred [1.79999974]: loss: [2.5590155e-07]\n",
            "Iter #439: y_pred [1.79999975]: loss: [2.45990308e-07]\n",
            "Iter #440: y_pred [1.79999976]: loss: [2.36462936e-07]\n",
            "Iter #441: y_pred [1.79999977]: loss: [2.27304565e-07]\n",
            "Iter #442: y_pred [1.79999978]: loss: [2.18500905e-07]\n",
            "Iter #443: y_pred [1.79999979]: loss: [2.10038216e-07]\n",
            "Iter #444: y_pred [1.7999998]: loss: [2.01903293e-07]\n",
            "Iter #445: y_pred [1.79999981]: loss: [1.9408344e-07]\n",
            "Iter #446: y_pred [1.79999981]: loss: [1.86566456e-07]\n",
            "Iter #447: y_pred [1.79999982]: loss: [1.7934061e-07]\n",
            "Iter #448: y_pred [1.79999983]: loss: [1.72394626e-07]\n",
            "Iter #449: y_pred [1.79999983]: loss: [1.65717664e-07]\n",
            "Iter #450: y_pred [1.79999984]: loss: [1.59299306e-07]\n",
            "Iter #451: y_pred [1.79999985]: loss: [1.53129535e-07]\n",
            "Iter #452: y_pred [1.79999985]: loss: [1.47198724e-07]\n",
            "Iter #453: y_pred [1.79999986]: loss: [1.41497617e-07]\n",
            "Iter #454: y_pred [1.79999986]: loss: [1.36017318e-07]\n",
            "Iter #455: y_pred [1.79999987]: loss: [1.30749274e-07]\n",
            "Iter #456: y_pred [1.79999987]: loss: [1.25685265e-07]\n",
            "Iter #457: y_pred [1.79999988]: loss: [1.20817389e-07]\n",
            "Iter #458: y_pred [1.79999988]: loss: [1.16138049e-07]\n",
            "Iter #459: y_pred [1.79999989]: loss: [1.11639943e-07]\n",
            "Iter #460: y_pred [1.79999989]: loss: [1.07316052e-07]\n",
            "Iter #461: y_pred [1.7999999]: loss: [1.03159628e-07]\n",
            "Iter #462: y_pred [1.7999999]: loss: [9.91641846e-08]\n",
            "Iter #463: y_pred [1.7999999]: loss: [9.53234878e-08]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruJrzWQg0lGp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "35af4edf-fec5-4e09-f481-fb2a5608d8bf"
      },
      "source": [
        "loss = pd.DataFrame({'train_loss': model['train_loss']})\n",
        "loss = loss.reset_index().rename(columns={'index': 'iter'})\n",
        "loss['iter'] += 1\n",
        "loss.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>train_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>[2.197182779535912]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>[2.1563008020517613]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>[2.1169091616709528]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>[2.078864112995721]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>[2.0420352616619186]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   iter            train_loss\n",
              "0     1   [2.197182779535912]\n",
              "1     2  [2.1563008020517613]\n",
              "2     3  [2.1169091616709528]\n",
              "3     4   [2.078864112995721]\n",
              "4     5  [2.0420352616619186]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6cl0qVq0n3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "outputId": "d0190297-2ec9-4df7-8557-6f833d0b2ba0"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=go.Scatter(x=loss['iter'], y=loss['train_loss'], mode='markers+lines'))\n",
        "fig.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"c3262999-0efa-4092-9757-ddaeb9c37099\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"c3262999-0efa-4092-9757-ddaeb9c37099\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'c3262999-0efa-4092-9757-ddaeb9c37099',\n",
              "                        [{\"mode\": \"markers+lines\", \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464], \"y\": [[2.197182779535912], [2.1563008020517613], [2.1169091616709528], [2.078864112995721], [2.0420352616619186], [2.0063042265853572], [1.9715634778983748], [1.937715324219522], [1.9046710271058038], [1.872350023999228], [1.8406792438429875], [1.8095925019224153], [1.7790299624733754], [1.748937659268926], [1.719267065802338], [1.6899747078778078], [1.661021812437285], [1.6323739873232612], [1.604000927427844], [1.5758761433279769], [1.5479767090714533], [1.520283026271594], [1.492778602100755], [1.4654498391528643], [1.4382858354799901], [1.4112781934031655], [1.3844208359580494], [1.3577098300651804], [1.3311432157157657], [1.3047208406396449], [1.2784442000744975], [1.2523162813863216], [1.2263414134023538], [1.200525120410365], [1.1748739808539856], [1.149395490813678], [1.124097932408384], [1.0989902472849529], [1.0740819153823957], [1.0493828391670108], [1.024903233533681], [1.0006535215593715], [0.9766442362782215], [0.9528859286248307], [0.9293890816645314], [0.9061640311977018], [0.8832208927906308], [0.8605694952490273], [0.8382193205129866], [0.8161794499148616], [0.7944585167048761], [0.773064664714104], [0.7520055129912027], [0.7312881262185471], [0.7109189906855364], [0.6909039955721412], [0.6712484192744397], [0.6519569204860987], [0.6330335337355004], [0.6144816690675272], [0.5963041155517843], [0.5785030482951143], [0.5610800386355195], [0.5440360671967404], [0.5273715394876151], [0.5110863037375826], [0.49517967066910695], [0.47965043491904846], [0.46449689783381554], [0.4497168913772531], [0.4353078029053128], [0.42126660057744636], [0.407589859191021], [0.39427378624174003], [0.3813142480298022], [0.3687067956481733], [0.35644669070574864], [0.34452893065415435], [0.3329482736024043], [0.32169926251846914], [0.31077624873093446], [0.3001734146573096], [0.28988479569808523], [0.27990430124735366], [0.27022573478165923], [0.2608428129987155], [0.2517491839867614], [0.24293844441359025], [0.23440415573174067], [0.22613985940299708], [0.21813909115122887], [0.21039539425777742], [0.20290233191807183], [0.1956534986820071], [0.18864253100385753], [0.18186311693019053], [0.17530900495643276], [0.16897401208445983], [0.16285203111487134], [0.1569370372085297], [0.15122309375252208], [0.14570435756596223], [0.14037508348105554], [0.13522962833461372], [0.13026245440476658], [0.12546813232699994], [0.12084134352289122], [0.11637688217401632], [0.11206965677251635], [0.10791469127873987], [0.10390712591523332], [0.10004221762517518], [0.09631534022211885], [0.09272198425668621], [0.08925775662458935], [0.08591837993912566], [0.08269969169003599], [0.0795976432094132], [0.07660829846413608], [0.07372783269312744], [0.07095253090661902], [0.06827878626347705], [0.06570309834158583], [0.06322207131527047], [0.06083241205274015], [0.058530928145609895], [0.05631452588164376], [0.05418020817102431], [0.052125072435634356], [0.05014630847006685], [0.04824119628236612], [0.046407103921798365], [0.044641485300328965], [0.04294187801385818], [0.041305901168712245], [0.039731253218349805], [0.03821570981475131], [0.03675712167848899], [0.035353412491052616], [0.03400257681259844], [0.032702678027921595], [0.03145184632310394], [0.03024827669497543], [0.029090226995227342], [0.02797601601075228], [0.026904021581527493], [0.025872678757139278], [0.02488047799283133], [0.023925963385768467], [0.023007730952032235], [0.02212442694471428], [0.021274746213314488], [0.020457430604533755], [0.019671267404428816], [0.01891508782178719], [0.01818776551249135], [0.017488215144550123], [0.016815391003411007], [0.016168285637080393], [0.01554592854054504], [0.014947384878915004], [0.014371754248674984], [0.01381816947639325], [0.013285795454196059], [0.012773828011295008], [0.012281492820833906], [0.011808044341293789], [0.011352764791689784], [0.01091496315977869], [0.01049397424248455], [0.010089157717755759], [0.009699897247053224], [0.009325599607674429], [0.008965693854122714], [0.008619630507733733], [0.008286880773775929], [0.007966935785247431], [0.007659305872608657], [0.007363519858694545], [0.00707912437805347], [0.006805683219992309], [0.0065427766945995725], [0.006290001021039826], [0.006046967737433517], [0.005813303131630754], [0.005588647692233328], [0.005372655579197083], [0.005164994113395771], [0.004965343284517587], [0.0047733952766984], [0.004588854011300603], [0.004411434706264927], [0.004240863451481225], [0.004076876799631757], [0.003919221371986525], [0.0037676534786321714], [0.0036219387526448266], [0.003481851797717761], [0.0033471758487830794], [0.0032177024451667258], [0.003093231115839812], [0.0029735690763450506], [0.002858530936974635], [0.0027479384218112024], [0.0026416200982366522], [0.0025394111165371047], [0.0024411529592420766], [0.002346693199849925], [0.0022558852705907295], [0.002168588238913527], [0.002084666592367501], [0.002003990031582026], [0.0019264332710418053], [0.0018518758473773378], [0.0017802019348909237], [0.0017113001680562068], [0.0016450634707305678], [0.0015813888918332353], [0.0015201774472524132], [0.0014613339677496118], [0.0014047669526326967], [0.0013503884289949308], [0.0012981138162992956], [0.0012478617961211302], [0.0011995541868443649], [0.0011531158231361527], [0.001108474440013385], [0.0010655605613358876], [0.001024307392551771], [0.0009846507175401698], [0.000946528799398827], [0.0009098822850213129], [0.0008746541133266561], [0.00084078942700061], [0.0008082354876157716], [0.0007769415940048763], [0.0007468590037598144], [0.0007179408577424606], [0.0006901421074914094], [0.0006634194454102627], [0.0006377312376397715], [0.0006130374595054722], [0.0005892996334446732], [0.0005664807693206431], [0.0005445453070251904], [0.0005234590612952505], [0.0005031891686477774], [0.0004837040363543377], [0.00046497329338213156], [0.00044696774322283694], [0.00042965931853511385], [0.0004130210375368204], [0.00039702696207388755], [0.0003816521573052345], [0.00036687265294355065], [0.0003526654059864409], [0.000339008264885754], [0.0003258799350993602], [0.0003132599459674257], [0.00030112861887166], [0.00028946703661802964], [0.00027825701400452374], [0.0002674810695193486], [0.00025712239813802107], [0.0002471648451656261], [0.00023759288108915477], [0.0002283915774037304], [0.00021954658336809096], [0.0002110441036615729], [0.0002028708769015175], [0.0001950141549906803], [0.00018746168326333468], [0.0001802016813983176], [0.00017322282506881947], [0.00016651422830116225], [0.00016006542651614275], [0.00015386636022651778], [0.0001479073593599889], [0.00014217912819591838], [0.00013667273087714094], [0.00013137957748532436], [0.00012629141065478855], [0.00012140029270280017], [0.00011669859325791343], [0.00011217897736659488], [0.00010783439406036877], [0.00010365806536483113], [9.964347573210297e-05], [9.578436188628636e-05], [9.207470305638843e-05], [8.850871159116203e-05], [8.508082393388072e-05], [8.178569195060881e-05], [7.861817458865161e-05], [7.557332986096732e-05], [7.264640714121917e-05], [6.983283975436905e-05], [6.712823785615107e-05], [6.452838158632623e-05], [6.20292144899448e-05], [5.962683718907513e-05], [5.731750130233415e-05], [5.5097603599785216e-05], [5.296368037988053e-05], [5.091240207222292e-05], [4.8940568040611865e-05], [4.7045101591924876e-05], [4.522304517773357e-05], [4.3471555780660864e-05], [4.178790048015024e-05], [4.016945218832113e-05], [3.861368555391209e-05], [3.711817302165521e-05], [3.568058104641558e-05], [3.429866645210389e-05], [3.297027293180932e-05], [3.169332768693245e-05], [3.046583818955284e-05], [2.928588907802343e-05], [2.8151639165363562e-05], [2.706131856933247e-05], [2.601322595041644e-05], [2.5005725856841465e-05], [2.4037246175057092e-05], [2.3106275675477406e-05], [2.2211361654589368e-05], [2.1351107669209668e-05], [2.052417135822715e-05], [1.9729262347167875e-05], [1.8965140235138733e-05], [1.823061266059689e-05], [1.752453343972782e-05], [1.6845800778764186e-05], [1.619335555536061e-05], [1.55661796645834e-05], [1.4963294432623897e-05], [1.4383759087133186e-05], [1.3826669292837934e-05], [1.3291155735783988e-05], [1.2776382771972905e-05], [1.228154711996332e-05], [1.1805876608983468e-05], [1.1348628974117148e-05], [1.0909090697452939e-05], [1.0486575895196637e-05], [1.0080425247638303e-05], [9.690004969309385e-06], [9.31470582243854e-06], [8.953942163492101e-06], [8.607151031236882e-06], [8.273791266999453e-06], [7.953342672228914e-06], [7.645305194481367e-06], [7.349198148931535e-06], [7.06455946608564e-06], [6.7909449741332395e-06], [6.527927702615344e-06], [6.275097219177184e-06], [6.032058985638855e-06], [5.798433744930165e-06], [5.5738569297858476e-06], [5.357978090314575e-06], [5.150460351988073e-06], [4.95097988539861e-06], [4.759225404216139e-06], [4.574897678022438e-06], [4.3977090657953966e-06], [4.227383066046642e-06], [4.063653887831364e-06], [3.906266032638328e-06], [3.7549738982622927e-06], [3.6095413948888933e-06], [3.4697415760565065e-06], [3.3353562856053287e-06], [3.2061758157286846e-06], [3.0819985816776807e-06], [2.9626308053476436e-06], [2.8478862146297246e-06], [2.737585750089977e-06], [2.631557289412001e-06], [2.5296353751702583e-06], [2.431660958590598e-06], [2.3374811510823434e-06], [2.2469489855403424e-06], [2.159923186750845e-06], [2.0762679511232562e-06], [1.9958527348595823e-06], [1.918552049895439e-06], [1.844245269166933e-06], [1.7728164367625254e-06], [1.704154088288945e-06], [1.6381510759000406e-06], [1.5747044022074164e-06], [1.5137150584099146e-06], [1.4550878710828385e-06], [1.3987313525198886e-06], [1.3445575577364366e-06], [1.2924819492443618e-06], [1.242423262937109e-06], [1.1943033815242643e-06], [1.1480472144054232e-06], [1.1035825788763276e-06], [1.0608400879963398e-06], [1.0197530411204525e-06], [9.80257322646949e-07], [9.422912989887067e-07], [9.05795724870373e-07], [8.707136480712308e-07], [8.369903230498466e-07], [8.045731250128085e-07], [7.734114662039104e-07], [7.434567192987629e-07], [7.146621392450925e-07], [6.869827926525574e-07], [6.603754856282507e-07], [6.347986969412744e-07], [6.102125145179826e-07], [5.865785712710903e-07], [5.638599864798977e-07], [5.420213076146041e-07], [5.210284552692457e-07], [5.00848670537124e-07], [4.814504623862348e-07], [4.628035596976332e-07], [4.4487886374788843e-07], [4.2764840357811806e-07], [4.110852909189333e-07], [3.9516367844605327e-07], [3.7985872114454367e-07], [3.6514653456443114e-07], [3.510041612919679e-07], [3.3740953142569197e-07], [3.2434143082404887e-07], [3.117794669105223e-07], [2.9970403603307716e-07], [2.8809629437631656e-07], [2.7693812909568294e-07], [2.6621212634303504e-07], [2.5590154950627664e-07], [2.4599030834515645e-07], [2.3646293612067382e-07], [2.2730456517017217e-07], [2.1850090470287853e-07], [2.1003821570886316e-07], [2.0190329252933736e-07], [1.9408344020810375e-07], [1.8656645628389867e-07], [1.7934061014024394e-07], [1.7239462568596764e-07], [1.6571766425776957e-07], [1.5929930596847441e-07], [1.5312953549617703e-07], [1.4719872387658484e-07], [1.4149761695669838e-07], [1.3601731763124292e-07], [1.3074927385225976e-07], [1.2568526508438538e-07], [1.2081738876013048e-07], [1.161380489556052e-07], [1.1163994284579815e-07], [1.0731605160074764e-07], [1.0315962772899923e-07], [9.916418464150922e-08], [9.532348776986055e-08]]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('c3262999-0efa-4092-9757-ddaeb9c37099');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsEEhYRP0qrZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1bf0bf2a-a6c4-489a-a5b5-aa05df59b9f5"
      },
      "source": [
        "predict(X, model['W1'], model['W2'])"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.80000008])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}